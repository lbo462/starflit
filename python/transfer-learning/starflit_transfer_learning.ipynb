{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starflit's transfer learning\n",
    "\n",
    "Welcome to the Starflit's transfer learning notebook!\n",
    "\n",
    "> Before diving into this, you're highly encouraged to take a look at the documentation about transfer learning on GitHub.\n",
    "\n",
    "This notebook was highly inspired by the [documentation from Keras](https://keras.io/guides/transfer_learning/).\n",
    "\n",
    "As said is the documentation, transfer learning are performed is two steps:\n",
    "- _feature extraction_, that will output a model that we'll call __tuned__;\n",
    "- _fine-tuning_, that will output a model that we'll call __fine-tuned__;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and importing a dataset\n",
    "\n",
    "This part, was probably the most difficult one, unexpectedly.\n",
    "\n",
    "As transfer learning is really nicely documented, the creation and the import of a dataset is not.\n",
    "We tried several approach, like using HuggingFace, but most of these approach failed.\n",
    "\n",
    "You'll surely find a bit more details about this in the documentation about transfer learning on GitHub.\n",
    "\n",
    "What we finally came up with was to import our dataset locally in a folder `dataset/` having the following structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    class1/\n",
    "        image1.JPG\n",
    "        image2.JPG\n",
    "        ...\n",
    "    class2/\n",
    "        image1.JPG\n",
    "        image2.JPG\n",
    "        ...\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset\n",
    "\n",
    "Now, one have to import this dataset.\n",
    "\n",
    "__If you're trying to run this notebook on Google Colab__,\n",
    "you'll need to mount your personal drive and change the current directory.\n",
    "\n",
    "> This implies that you've uploaded your dataset to your personal drive.\n",
    "\n",
    "In the following codeblock, edit the path to match the path to your dataset.\n",
    "You can check the files structures by clicking files and searching for the folder named content at the root of the file system.\n",
    "Here, I placed my `dataset` folder in a folder named Colab Notebooks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "We're working in a notebook here, so we don't have to bother with virtual environment.\n",
    "\n",
    "This also means that we don't have a clear list of dependencies such as `requirements.txt`.\n",
    "\n",
    "Hence, we install the dependencies as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not hesitate to edit the list of dependencies.\n",
    "\n",
    "And just for the record, here are the dependencies we used we actually trained the model:\n",
    "\n",
    "```\n",
    "absl-py==2.1.0\n",
    "astunparse==1.6.3\n",
    "certifi==2024.6.2\n",
    "charset-normalizer==3.3.2\n",
    "flatbuffers==24.3.25\n",
    "gast==0.5.4\n",
    "google-pasta==0.2.0\n",
    "grpcio==1.64.1\n",
    "h5py==3.11.0\n",
    "idna==3.7\n",
    "keras==3.3.3\n",
    "libclang==18.1.1\n",
    "Markdown==3.6\n",
    "markdown-it-py==3.0.0\n",
    "MarkupSafe==2.1.5\n",
    "mdurl==0.1.2\n",
    "ml-dtypes==0.3.2\n",
    "namex==0.0.8\n",
    "numpy==1.26.4\n",
    "nvidia-cublas-cu12==12.3.4.1\n",
    "nvidia-cuda-cupti-cu12==12.3.101\n",
    "nvidia-cuda-nvcc-cu12==12.3.107\n",
    "nvidia-cuda-nvrtc-cu12==12.3.107\n",
    "nvidia-cuda-runtime-cu12==12.3.101\n",
    "nvidia-cudnn-cu12==8.9.7.29\n",
    "nvidia-cufft-cu12==11.0.12.1\n",
    "nvidia-curand-cu12==10.3.4.107\n",
    "nvidia-cusolver-cu12==11.5.4.101\n",
    "nvidia-cusparse-cu12==12.2.0.103\n",
    "nvidia-nccl-cu12==2.19.3\n",
    "nvidia-nvjitlink-cu12==12.3.101\n",
    "opt-einsum==3.3.0\n",
    "optree==0.11.0\n",
    "packaging==24.1\n",
    "pillow==10.3.0\n",
    "protobuf==4.25.3\n",
    "Pygments==2.18.0\n",
    "requests==2.32.3\n",
    "rich==13.7.1\n",
    "scipy==1.13.1\n",
    "six==1.16.0\n",
    "tensorboard==2.16.2\n",
    "tensorboard-data-server==0.7.2\n",
    "tensorflow==2.16.1\n",
    "tensorflow-io-gcs-filesystem==0.37.0\n",
    "termcolor==2.4.0\n",
    "typing_extensions==4.12.2\n",
    "urllib3==2.2.2\n",
    "Werkzeug==3.0.3\n",
    "wrapt==1.16.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we import all we need here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow import lite as tflite\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Feature extraction is the first step of transfer learning.\n",
    "\n",
    "Here, we'll preprocess our images and split them into different splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qsi1M0zEdby",
    "outputId": "b3051012-8dab-42c5-fd7f-df1e28dde781"
   },
   "outputs": [],
   "source": [
    "# Preprocessing and data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.5  # Keep only a part of the dataset, represented as a percent (between 0 and 1)\n",
    ")\n",
    "\n",
    "# Extract train dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './dataset',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Extract validation dataset\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    './dataset',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two splits, data augmented.\n",
    "\n",
    "Our next objective is to load the model we have to train and add some layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svRP2HdjEodx"
   },
   "outputs": [],
   "source": [
    "# Load the MobileNetV3Large model with the imagenet weights as initial weights\n",
    "base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Now, we add our custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that `include_top=False` means that we're not including the classification layer of the base model.\n",
    "> That's exactly what allows us to have our customs classes as output.\n",
    "\n",
    "As described in the Keras documentation, one have to freeze all the layers of the base model (but not the layers we just added).\n",
    "\n",
    "When performing feature extraction, we only train the layers we added !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model to take the freeze into account\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare an early stopping, so that the training will stop if no improvement is to occur between three consecutive epochs.\n",
    "\n",
    "We also prepare a [model checkpoint](https://www.tensorflow.org/tutorials/keras/save_and_load#save_checkpoints_during_training) callback\n",
    "so that we will be able to continue our training even the execution stops randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='checkpoints/cp.weights.h5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__IF THE PREVIOUS TRAINING WAS INTERRUPTED__, you can load the previous weights with this:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.load_weights('checkpoints/cp.ckpt')\n",
    "\n",
    "print(\"Model restored, evaluating ...\")\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "\n",
    "print(f'Validation loss: {loss}')\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now is the time we actually train the model.\n",
    "\n",
    "The following codeblock will probably last for long, and that's why we have trouble running this notebook where we want :'("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rtcr61eVEtm-",
    "outputId": "7e70ef61-82d5-41f4-993f-90bcc5bb87b1"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=1000,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we have our trained model, tuned to our dataset.\n",
    "Feature extraction is over, and we'll proceed to _fine-tuning_ is few seconds,\n",
    "just after a quick model save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mobilenetv3_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tuned model\n",
    "\n",
    "Note that the model is actually tuned for our dataset.\n",
    "Hence, now is a good timing to have some testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "\n",
    "print(f'Validation loss: {loss}')\n",
    "print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "This step is optional, but we actually want to do a round of fine-tuning on the whole model.\n",
    "\n",
    "This is done via unfreezing every layer of the model and re-run a training with a small learning rate.\n",
    "\n",
    "> It's important to keep a low learning rate!\n",
    "> The danger is overfitting the model here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LYGW40dNJjI",
    "outputId": "666300d1-0437-4630-c01d-126479fed99f"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model, with a low learning rate (1e-5 here)\n",
    "model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we re-train the model.\n",
    "This is also kind of long to run, same as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=1000,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's re-save our model a second time, but fine-tuned this time.\n",
    "This is the model we'll be using in our beloved Strandbeest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5XNVGcZbR6_",
    "outputId": "1c06be88-932f-46eb-cdc9-97941b3157b0"
   },
   "outputs": [],
   "source": [
    "model.save('mobilenetv3_fine_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing fine-tuned model\n",
    "\n",
    "Now, everything is set to have some little performance tests, using the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVkDQ-QlV3Sr",
    "outputId": "7abec03b-5e38-4b47-918a-3130cdf0a684"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "\n",
    "print(f'Validation loss: {loss}')\n",
    "print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "acc = history_fine.history['accuracy']\n",
    "val_acc = history_fine.history['val_accuracy']\n",
    "loss = history_fine.history['loss']\n",
    "val_loss = history_fine.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convert the model to tflite\n",
    "\n",
    "All the models we have exported are under the Keras models format : `.h5`.\n",
    "\n",
    "But our strandbeest should be fed with `.tflite` files.\n",
    "Thus, we convert our model here:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert tuned model\n",
    "converter = tflite.TFLiteConverter.from_keras_model_file('mobilenetv3_tuned.h5')\n",
    "model = converter.convert()\n",
    "with open('mobilenetv3_tuned.tflite' , 'wb') as f:\n",
    "    f.write(model)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert fine-tuned model\n",
    "converter = tflite.TFLiteConverter.from_keras_model_file('mobilenetv3_fine_tuned.h5')\n",
    "model = converter.convert()\n",
    "with open('mobilenetv3_fine_tuned.tflite' , 'wb') as f:\n",
    "    f.write(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
